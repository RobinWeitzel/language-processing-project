{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image sentiment\n",
    "Now that I have labeled images I can create a model to train on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.0.1\n",
      "Torchvision Version:  0.2.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 200\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 8\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '../Datasets/predictions.json'\n",
    "tweets = pd.read_json(file_path, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[[3,5]]\n",
    "tweets = tweets.groupby(3).mean()\n",
    "tweets['path'] = tweets.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out tweets with broken images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 9549/32947 [01:22<03:10, 123.02it/s]C:\\Users\\Robin\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:952: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  ' expressed in bytes should be converted ' +\n",
      "100%|██████████| 32947/32947 [04:37<00:00, 118.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pretrainedmodels.utils as utils\n",
    "\n",
    "load_img = utils.LoadImage()\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "tweets['ok'] = 1\n",
    "\n",
    "for index, row in tqdm(tweets.iterrows(), total=len(tweets)):\n",
    "    try:\n",
    "        img = load_img(row['path'])\n",
    "        img = transform(img)\n",
    "    except:\n",
    "        tweets.at[index, 'ok'] = 0\n",
    "        \n",
    "tweets = tweets[tweets['ok'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_acc2 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_corrects2 = 0\n",
    "            \n",
    "            t = tqdm(dataloaders[phase], mininterval=1, desc='-(' + phase + ')', leave=False)\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in t:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    preds = model(inputs)\n",
    "                    loss = criterion(preds, labels.squeeze_())\n",
    "                    description = \"Loss: \" + str(loss.item())\n",
    "                    t.set_description(description)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()   \n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                for i in range(len(preds)):\n",
    "                    pred = preds[i]\n",
    "                    label = labels.data[i]\n",
    "                    \n",
    "                    if(torch.abs(pred-label) < 0.2):\n",
    "                        running_corrects += 1\n",
    "                        \n",
    "                    if(pred < 0.5 and label < 0.5 or pred > 0.5 and label > 0.5):\n",
    "                        running_corrects2 += 1\n",
    "                    \n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            epoch_acc2 = running_corrects2 / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} Acc2: {:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_acc2))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_acc2 = epoch_acc2\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f}  Acc2: {:.4f}'.format(best_acc, best_acc2))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): AlexNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): ReLU(inplace)\n",
      "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace)\n",
      "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU(inplace)\n",
      "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace)\n",
      "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.5)\n",
      "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): Dropout(p=0.5)\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU(inplace)\n",
      "      (6): Linear(in_features=4096, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "    num_ftrs = model_ft.classifier[6].in_features\n",
    "    model_ft.classifier[6] = nn.Linear(num_ftrs, 1)\n",
    "    model_ft = nn.Sequential(\n",
    "        model_ft,\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    input_size = 224\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import codecs\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class LoadData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.load_img = utils.LoadImage()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.data = data.values\n",
    "\n",
    "    def __getitem__(self, index):   \n",
    "        path_img = self.data[index][1]\n",
    "        sentiment = self.data[index][0]\n",
    "        img = self.load_img(path_img)\n",
    "        img = self.transform(img)\n",
    "\n",
    "        sentiment = torch.tensor([sentiment], dtype=torch.float32)\n",
    "\n",
    "        return img, sentiment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Split dataset\n",
    "train_data = tweets.sample(frac=0.8)\n",
    "test_data = tweets.loc[~tweets.index.isin(train_data.index), :]\n",
    "\n",
    "# Depending on what I want to classifier I can either use the default fold erstructure or need to use my own data loader.\n",
    "image_datasets = {\n",
    "    'train': LoadData(train_data),\n",
    "    'val': LoadData(test_data)\n",
    "}\n",
    "\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "useGPU = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and useGPU else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t 0.features.0.weight\n",
      "\t 0.features.0.bias\n",
      "\t 0.features.3.weight\n",
      "\t 0.features.3.bias\n",
      "\t 0.features.6.weight\n",
      "\t 0.features.6.bias\n",
      "\t 0.features.8.weight\n",
      "\t 0.features.8.bias\n",
      "\t 0.features.10.weight\n",
      "\t 0.features.10.bias\n",
      "\t 0.classifier.1.weight\n",
      "\t 0.classifier.1.bias\n",
      "\t 0.classifier.4.weight\n",
      "\t 0.classifier.4.bias\n",
      "\t 0.classifier.6.weight\n",
      "\t 0.classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/7\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0593 Acc: 0.5951 Acc2: 0.7444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0539 Acc: 0.5998 Acc2: 0.7638\n",
      "\n",
      "Epoch 1/7\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0550 Acc: 0.6045 Acc2: 0.7685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0526 Acc: 0.6010 Acc2: 0.7689\n",
      "\n",
      "Epoch 2/7\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0537 Acc: 0.6025 Acc2: 0.7726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0519 Acc: 0.6042 Acc2: 0.7720\n",
      "\n",
      "Epoch 3/7\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0531 Acc: 0.6059 Acc2: 0.7752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0517 Acc: 0.6024 Acc2: 0.7714\n",
      "\n",
      "Epoch 4/7\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0526 Acc: 0.6065 Acc2: 0.7760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0513 Acc: 0.6027 Acc2: 0.7718\n",
      "\n",
      "Epoch 5/7\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0523 Acc: 0.6068 Acc2: 0.7766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0510 Acc: 0.6117 Acc2: 0.7720\n",
      "\n",
      "Epoch 6/7\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0521 Acc: 0.6071 Acc2: 0.7774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0510 Acc: 0.6038 Acc2: 0.7718\n",
      "\n",
      "Epoch 7/7\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0519 Acc: 0.6056 Acc2: 0.7772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0508 Acc: 0.6086 Acc2: 0.7721\n",
      "\n",
      "Training complete in 34m 42s\n",
      "Best val Acc: 0.6117  Acc2: 0.7720\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'final_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarcasm detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def predict(data_iterator, model):\n",
    "    model.eval()\n",
    "    t = tqdm(data_iterator, mininterval=1, desc='-(Prediction)', leave=False)\n",
    "    \n",
    "    test_data['sentiment'] = 0.5\n",
    "    values = test_data.values\n",
    "    counter = 0\n",
    "    \n",
    "    for batch in t:\n",
    "        image, score = batch\n",
    "        if torch.cuda.is_available() and useGPU:\n",
    "            image = Variable(image.cuda())\n",
    "            \n",
    "        pred = model(image)\n",
    "        pred = pred.cpu()\n",
    "        for p in pred:\n",
    "            values[counter][3] = p.item()\n",
    "            counter += 1\n",
    "        \n",
    "    return pd.DataFrame(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):   0%|          | 0/33 [00:00<?, ?it/s]C:\\Users\\Robin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):   3%|▎         | 1/33 [00:01<00:36,  1.14s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):   9%|▉         | 3/33 [00:02<00:31,  1.07s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):  15%|█▌        | 5/33 [00:04<00:27,  1.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):  21%|██        | 7/33 [00:06<00:25,  1.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):  27%|██▋       | 9/33 [00:08<00:22,  1.08it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):  33%|███▎      | 11/33 [00:09<00:19,  1.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):  39%|███▉      | 13/33 [00:11<00:17,  1.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):  45%|████▌     | 15/33 [00:13<00:15,  1.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):  52%|█████▏    | 17/33 [00:14<00:13,  1.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):  58%|█████▊    | 19/33 [00:17<00:13,  1.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):  64%|██████▎   | 21/33 [00:19<00:11,  1.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):  70%|██████▉   | 23/33 [00:21<00:09,  1.04it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):  76%|███████▌  | 25/33 [00:22<00:07,  1.07it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):  82%|████████▏ | 27/33 [00:25<00:06,  1.01s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):  88%|████████▊ | 29/33 [00:26<00:03,  1.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction):  94%|█████████▍| 31/33 [00:28<00:01,  1.07it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-(Prediction): 100%|██████████| 33/33 [00:30<00:00,  1.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                              "
     ]
    }
   ],
   "source": [
    "predictions = predict(dataloaders_dict['val'], model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1501"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opposing labels\n",
    "len(predictions[round(predictions[0].astype(float)) - round(predictions[3].astype(float)) != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Great difference\n",
    "len(predictions[abs(predictions[0] - predictions[3]) > 0.5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
